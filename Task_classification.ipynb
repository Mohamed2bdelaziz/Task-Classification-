{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyNPLpUTNC5dH/F1iplHEVR7",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Mohamed2bdelaziz/Task-Classification-/blob/main/Task_classification.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FGs3W2O4Jnt3",
        "outputId": "03a1bc84-f611-4564-cc65-31e6b9207fc8"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "from sklearn.metrics.pairwise import cosine_similarity\n",
        "import nltk\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "from gensim.models import KeyedVectors"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! chmod 600 /root/.kaggle/kaggle.json"
      ],
      "metadata": {
        "id": "JrpAPk-dLh6R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "! kaggle datasets download -d thanakomsn/glove6b300dtxt"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "QgpgC_kmJ0yy",
        "outputId": "254d610f-b942-4ff6-f255-c07f68207c2b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading glove6b300dtxt.zip to /content\n",
            " 99% 384M/386M [00:16<00:00, 29.2MB/s]\n",
            "100% 386M/386M [00:16<00:00, 24.5MB/s]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "! unzip /content/glove6b300dtxt.zip"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zrg_S7IZLSn-",
        "outputId": "08243ebe-907b-4232-b246-c632f0b8c430"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Archive:  /content/glove6b300dtxt.zip\n",
            "  inflating: glove.6B.300d.txt       \n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Load pre-trained GloVe word embeddings\n",
        "glove_path = '/content/glove.6B.300d.txt'\n",
        "# Function to load the GloVe word vectors\n",
        "def load_glove_word_vectors(glove_path):\n",
        "    word_vectors = {}\n",
        "    with open(glove_path, 'r', encoding='utf-8') as f:\n",
        "        for line in f:\n",
        "            values = line.split()\n",
        "            word = values[0]\n",
        "            vector = np.asarray(values[1:], dtype='float32')\n",
        "            word_vectors[word] = vector\n",
        "    return word_vectors\n",
        "\n",
        "word_vectors = load_glove_word_vectors(glove_path)\n"
      ],
      "metadata": {
        "id": "Hyg-wc2lMXlh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to calculate the sentence embedding\n",
        "def calculate_sentence_embedding(sentence):\n",
        "    # Tokenize the sentence and remove stopwords\n",
        "    tokens = [token.lower() for token in word_tokenize(sentence) if token.lower() not in stopwords.words('english')]\n",
        "    # Get word embeddings for each token\n",
        "    embeddings = [word_vectors[token] for token in tokens if token in word_vectors]\n",
        "    # Calculate the average embedding for the sentence\n",
        "    if len(embeddings) > 0:\n",
        "        sentence_embedding = np.mean(embeddings, axis=0)\n",
        "        return sentence_embedding\n",
        "    else:\n",
        "        return None\n",
        "\n",
        "# Function to measure the distance between a sentence and a word using cosine similarity\n",
        "def measure_distance(sentence, word):\n",
        "    # Calculate the sentence embedding\n",
        "    sentence_embedding = calculate_sentence_embedding(sentence)\n",
        "    if sentence_embedding is not None:\n",
        "        # Get the word embedding\n",
        "        word = word.lower()\n",
        "        word_embedding = word_vectors[word]\n",
        "        # Calculate cosine similarity between the sentence and word embeddings\n",
        "        similarity = cosine_similarity([sentence_embedding], [word_embedding])[0][0]\n",
        "        # Distance is (1 - similarity)\n",
        "        distance = 1 - similarity\n",
        "        return distance\n",
        "    else:\n",
        "        return None\n"
      ],
      "metadata": {
        "id": "DcX2a8rDM_wn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "sentence = \"Study lec 11 reasoning course\"\n",
        "\n",
        "classes = [\"Personal\", \"Work\", \"Technology\", \"Family\", \"colage\"]\n",
        "\n",
        "\n",
        "# distance = measure_distance(sentence, \"person\")\n",
        "# distance\n",
        "distances = np.array([measure_distance(sentence, clas) for clas in classes])\n",
        "\n",
        "distances = np.exp(distances) / np.sum(np.exp(distances))\n",
        "\n",
        "print(classes[np.argmax(distances)])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "c3hg6o41OQrg",
        "outputId": "618661bd-a4dd-45fa-91b7-4cb53f48efcd"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "colage\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "list(zip(classes, distances))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zJ_506NjPXUw",
        "outputId": "9a660d77-e336-47b0-cae0-964784d79a95"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[('Personal', 0.18763471132810858),\n",
              " ('Work', 0.15533979839454784),\n",
              " ('Technology', 0.18218493231315616),\n",
              " ('Family', 0.2011060606895541),\n",
              " ('colage', 0.27373449727463345)]"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "G1BnyOHySlwK"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}